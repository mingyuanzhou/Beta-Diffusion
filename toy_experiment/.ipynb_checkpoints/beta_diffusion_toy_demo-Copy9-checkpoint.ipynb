{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "550859d0-7841-4531-ad42-9f19e0d8244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Options = [(\"Gauss\",\"Weighted_ELBO\"),(\"Beta\",\"ELBO\"),(\"Beta\",\"KLUB\"),(\"Beta\",\"KLUB_conditional\"),(\"Beta\",\"KLUB_marginal\"),(\"Gauss\",\"KLUB\")]\n",
    "Data_Choices = [\"Toy1\",\"Toy2\"]\n",
    "\n",
    "'''\n",
    "To reproduce the results in the paper, run the demo code with \"diffusion_option,loss_option = Options[i]\", where i=0,1,2,3,4, and with \"data_choice = Data_Choices[j]\", where j=0,1\n",
    "'''\n",
    "\n",
    "diffusion_option,loss_option = Options[3]\n",
    "data_choice = Data_Choices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae5b1ea8-b82a-4ec0-846e-02eccf14c4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from torch.distributions import Gamma\n",
    "from torch.distributions import Beta\n",
    "\n",
    "MIN = torch.finfo(torch.float32).tiny\n",
    "EPS = torch.finfo(torch.float32).eps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aa89ce8-ea16-416f-900d-1222622a8c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "SaveCheckPoint  = True\n",
    "\n",
    "\n",
    "\n",
    "#predict x0 or noise in Gaussian diffusion\n",
    "Predict_x0 = False\n",
    "    \n",
    "#Concentration parameter in beta diffusion \n",
    "eta = torch.tensor(10000,dtype=torch.float32)\n",
    "\n",
    "Beta_Linear_Schedule = True\n",
    "\n",
    "if Beta_Linear_Schedule ==False:\n",
    "    #Define sigmoid scheduling start, end, and power: alpha_t= torch.sigmoid(sigmoid_start+(sigmoid_end-sigmoid_start)*t**sigmoid_power)\n",
    "    sigmoid_start = 9\n",
    "    sigmoid_end = -9\n",
    "    sigmoid_power = 0.5\n",
    "else:\n",
    "    #Define beta linear schedule \n",
    "    beta_max = 20\n",
    "    beta_min= 0.1\n",
    "    beta_d = beta_max-beta_min\n",
    "\n",
    "\n",
    "#Position embedding dimension\n",
    "embed_size = 20\n",
    "\n",
    "#NFEs, the number of steps in reverse diffusion\n",
    "T = 200\n",
    "\n",
    "#BatchSize\n",
    "BatchSize=1000\n",
    "\n",
    "#for beta diffusion, the output of the generator NN is sent to sigmoid, and then scaled by Scale and shifted by Shift, so the output is always between [Shift, Shift+Scale]\n",
    "#Scale=0.39\n",
    "#Shift=0.60\n",
    "#Scale=0.8\n",
    "#Shift=0.1\n",
    "Scale=1\n",
    "Shift=0\n",
    "#The data values are assumed to lie within the range (0,1); If not, please adjust Scale and Shift accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0346e134-86d0-4c80-bb56-4479d6b38b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40daaf49-96cd-4078-8b43-c22e6e57bcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define functions\n",
    "def log_gamma(alpha):\n",
    "    return torch.log(torch._standard_gamma(alpha.to(torch.float32)))\n",
    "def KL_gamma(*args):\n",
    "    \"\"\"\n",
    "    Calculates the KL divergence between two Gamma distributions.\n",
    "    args[0]: alpha_p, the shape of the first Gamma distribution Gamma(alpha_p,beta_p).\n",
    "    args[1]: alpha_q,the shape of the second Gamma distribution Gamma(alpha_q,beta_q).\n",
    "    args[2]: beta_p, the rate (inverse scale) of the first Gamma distribution Gamma(alpha_p,beta_p).\n",
    "    args[3]: beta_q, the rate (inverse scale) of the second Gamma distribution Gamma(alpha_q,beta_q).\n",
    "    \"\"\"    \n",
    "    alpha_p = args[0]\n",
    "    alpha_q = args[1]\n",
    "    KL = (alpha_p-alpha_q)*torch.digamma(alpha_p)-torch.lgamma(alpha_p)+torch.lgamma(alpha_q)\n",
    "    if len(args)>2:\n",
    "        beta_p = args[2]\n",
    "        beta_q = args[3]\n",
    "        KL = KL + alpha_q*(torch.log(beta_p)-torch.log(beta_q))+alpha_p*(beta_q/beta_p-1.0)  \n",
    "    return KL\n",
    "\n",
    "def KL_beta(alpha_p,beta_p,alpha_q,beta_q):\n",
    "    \"\"\"\n",
    "    Calculates the KL divergence between two Beta distributions\n",
    "    KL(Beta(alpha_p,beta_p) || Beta(alpha_q,beta_q))\n",
    "    \"\"\"\n",
    "    KL =KL_gamma(alpha_p,alpha_q)+KL_gamma(beta_p,beta_q)-KL_gamma(alpha_p+beta_p,alpha_q+beta_q)\n",
    "    return KL\n",
    "\n",
    "def get_positional_embedding(embed_size, positions,alpha=None):\n",
    "    \"\"\"\n",
    "    positions is a tensor whose values are between 0 and 1\n",
    "    if embed_size=0, then return alpha as positional embedding\n",
    "    \"\"\"\n",
    "    #positions = torch.linspace(0, 1, seq_length)\n",
    "    #positions = positions.unsqueeze(1)\n",
    "    \n",
    "    #positions = -positions.logit()/8/1000\n",
    "    \n",
    "    seq_length = len(positions)\n",
    "    angles = 1000*positions / torch.pow(10000, torch.arange(0, embed_size, 2).float() / embed_size)\n",
    "    embeddings = torch.zeros(seq_length, embed_size)\n",
    "    if embed_size>0:\n",
    "        embeddings[:, 0::2] = torch.sin(angles)\n",
    "        embeddings[:, 1::2] = torch.cos(angles)\n",
    "    if alpha==None:\n",
    "        #embeddings[:,-1] = positions.squeeze(1)\n",
    "        return embeddings\n",
    "    else:\n",
    "        return torch.cat([embeddings,alpha],dim=1)\n",
    "    \n",
    "\n",
    "    \n",
    "def gauss_reverse_sampler(model,datashape,num_steps,alpha,embed_size,alpha_min=None):\n",
    "    \n",
    "    if 1:\n",
    "        step_indices = torch.arange(num_steps)\n",
    "        t_steps = 1-step_indices / (num_steps - 1)*(1-1e-5)\n",
    "        t_steps = torch.cat([t_steps, torch.zeros_like(t_steps[:1])])\n",
    "    else:\n",
    "        t_steps = 1- (torch.arange(0,num_steps+1)+1e-3)/(num_steps+2*1e-3)\n",
    "\n",
    "    \n",
    "    if Beta_Linear_Schedule==False:\n",
    "        logit_alpha = sigmoid_start + (sigmoid_end-sigmoid_start) * (t_steps**sigmoid_power)\n",
    "    else:\n",
    "        logit_alpha = (-0.5*beta_d*t_steps**2-beta_min*t_steps).to(torch.float64).exp().logit()\n",
    "    \n",
    "    alpha = logit_alpha.sigmoid()\n",
    "    \n",
    "    \n",
    "    if alpha_min is None:\n",
    "        alpha_min = alpha[0]\n",
    "        \n",
    "    x = torch.randn(datashape)\n",
    "           \n",
    "    #embedding_pos = alpha.unsqueeze(1)\n",
    "    embedding_pos = t_steps.unsqueeze(1)\n",
    "    \n",
    "    pe = get_positional_embedding(embed_size, embedding_pos)\n",
    "    \n",
    "    for i, (logit_alpha_cur,logit_alpha_next) in enumerate(zip(logit_alpha[:-1], logit_alpha[1:])): # 0, ..., N-1\n",
    "        \n",
    "        alpha_cur = logit_alpha_cur.sigmoid()\n",
    "        alpha_next = logit_alpha_next.sigmoid()\n",
    "        \n",
    "        embedding = pe[i].repeat(x.shape[0], 1)\n",
    "        with torch.no_grad(): \n",
    "            if Predict_x0:\n",
    "                x0_hat = model(x, embedding)\n",
    "            else:\n",
    "                noise_pred = model(x,embedding) \n",
    "                x0_hat = ((x - torch.sqrt(1-alpha_cur)*noise_pred)/torch.sqrt(alpha_cur))\n",
    "                \n",
    "            x = torch.sqrt(alpha_next)/(1-alpha_cur)*(1-alpha_cur/alpha_next)*x0_hat\\\n",
    "                    +(1-alpha_next)/(1-alpha_cur)*torch.sqrt(alpha_cur/alpha_next)*x\\\n",
    "                    +torch.sqrt((1-alpha_next)/(1-alpha_cur)*(1-alpha_cur/alpha_next))*torch.randn_like(x)\n",
    "    z_0 = x\n",
    "    return x0_hat, z_0    \n",
    "\n",
    "    \n",
    "def beta_reverse_sampler(model,datashape,num_steps,alpha,diffusion_option,Scale,Shift,embed_size,alpha_min=None,x_0_prior=None):\n",
    "    \n",
    "    if 1:\n",
    "        step_indices = torch.arange(num_steps)\n",
    "        t_steps = 1-step_indices / (num_steps - 1)*(1-1e-5)\n",
    "        t_steps = torch.cat([t_steps, torch.zeros_like(t_steps[:1])])\n",
    "    else:\n",
    "        t_steps = 1- (torch.arange(0,num_steps+1)+1e-3)/(num_steps+2*1e-3)\n",
    "        \n",
    "    \n",
    "    if Beta_Linear_Schedule==False:\n",
    "        logit_alpha = sigmoid_start + (sigmoid_end-sigmoid_start) * (t_steps**sigmoid_power)\n",
    "    else:\n",
    "        #logit_alpha = -0.5*beta_d*t_steps**2-beta_min*t_steps\n",
    "        logit_alpha =(-0.5*beta_d*t_steps**2-beta_min*t_steps).to(torch.float64).exp().logit()\n",
    "    \n",
    "    alpha = logit_alpha.sigmoid()\n",
    "    if alpha_min is None:\n",
    "        alpha_min = alpha[0]\n",
    "    if x_0_prior is None:\n",
    "        latents = torch.ones(datashape)/2*Scale+Shift\n",
    "        #latents = Beta(torch.ones(datashape),1.0).sample()*Scale+Shift\n",
    "    else:\n",
    "        latents = x_0_prior\n",
    "    \n",
    "    log_u = log_gamma( (eta * alpha_min * latents).to(torch.float32) )\n",
    "    log_v = log_gamma( (eta - eta * alpha_min * latents).to(torch.float32) )\n",
    "    x_next = (log_u - log_v)\n",
    "    \n",
    "    #embedding_pos = alpha.unsqueeze(1)\n",
    "    embedding_pos = t_steps.unsqueeze(1)\n",
    "    \n",
    "    \n",
    "    pe = get_positional_embedding(embed_size, embedding_pos)\n",
    "    for i, (logit_alpha_cur,logit_alpha_next) in enumerate(zip(logit_alpha[:-1], logit_alpha[1:])): # 0, ..., N-1\n",
    "        x_cur = x_next\n",
    "        alpha_cur = logit_alpha_cur.sigmoid()\n",
    "        alpha_next = logit_alpha_next.sigmoid()\n",
    "        \n",
    "        embedding = pe[i].repeat(x_next.shape[0], 1)\n",
    "        with torch.no_grad(): \n",
    "            logit_x0_hat = model(x_cur, embedding)\n",
    "\n",
    "        x0_hat = torch.sigmoid(logit_x0_hat) * Scale + Shift\n",
    "\n",
    "        alpha_reverse = (eta*alpha_next-eta*alpha_cur)*x0_hat\n",
    "        beta_reverse = eta-eta*alpha_next*x0_hat\n",
    "        \n",
    "        log_u = log_gamma(alpha_reverse.to(torch.float32))\n",
    "        log_v = log_gamma(beta_reverse.to(torch.float32))\n",
    "\n",
    "        concatenated = torch.cat((x_cur.unsqueeze(-1), (log_u-log_v).unsqueeze(-1), (x_cur+log_u-log_v).unsqueeze(-1)), dim=1)\n",
    "        x_next = torch.logsumexp(concatenated, dim=1)\n",
    "        \n",
    "   \n",
    "    out = (x0_hat- Shift) / Scale\n",
    "    out1 = ((torch.sigmoid(x_next)/alpha_next- Shift) / Scale) #.clamp(0,1)\n",
    "    return out, out1\n",
    "\n",
    "    \n",
    "def draw_minibatch(BatchSize=1000,data_choice=\"Toy1\"):\n",
    "    D = torch.tensor((1.0/7,2.0/7,3.0/7,4.0/7,5.0/7))\n",
    "    if data_choice == \"Toy1\":\n",
    "        \"\"\"\n",
    "        Toy data 1\n",
    "        \"\"\"    \n",
    "        Type = torch.randint(0,5,(BatchSize,1))\n",
    "        x0 = D[Type]\n",
    "    elif data_choice == \"Toy2\":\n",
    "        \"\"\"\n",
    "        Toy data 2\n",
    "        \"\"\"    \n",
    "        Type = torch.randint(0,5,(BatchSize,1))\n",
    "        x0 = torch.zeros(Type.shape)\n",
    "        x0[Type==0] = (Beta(torch.ones((Type==0).sum()),1).sample()*0.1+0.1)\n",
    "        x0[Type==1] = (Beta(torch.ones((Type==1).sum()),5).sample()*0.1+0.3)\n",
    "        x0[Type==2] = 0.5\n",
    "        x0[Type==3] = (Beta(torch.ones((Type==3).sum())*.5,.5).sample()*0.1+0.6)\n",
    "        x0[Type==4] = 0.8\n",
    "    else:\n",
    "        print(\"invalid Toy data choice\")\n",
    "    return x0, D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07628890-a8c2-41e8-8d24-7232d2063b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "MLP(\n",
      "  (fc1): Linear(in_features=21, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (activation1): ReLU()\n",
      "  (activation2): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "# Get cpu or gpu device for training.\n",
    "#device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=1+embed_size, hidden_size=256, output_size=1):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.activation2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, logit_x_t, pe):\n",
    "        # Input1 is a regular tensor of dimension 1\n",
    "        # Input2 is a positional embedding tensor of dimension 20     \n",
    "        x = self.fc1(torch.cat([logit_x_t,pe],dim=1))\n",
    "        x = self.activation1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    # def weight_initial(self,m):\n",
    "    #     if isinstance(m, nn.Linear):\n",
    "    #         nn.init.xavier_uniform_(m.weight)\n",
    "    #         nn.init.zeros_(m.bias)\n",
    "        \n",
    "model = MLP().to(device)\n",
    "print(model)\n",
    "\n",
    "lr  = torch.tensor(5e-4)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "if False:\n",
    "    #optional lr scheduler; disabled by default\n",
    "    from torch.optim.lr_scheduler import StepLR\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184b242e-3815-4021-ab32-64e1dec31eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Beta linear noise scheduling\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5)) #, sharey=True)\n",
    "position = torch.linspace(0, 1, 100)\n",
    "print(position)\n",
    "#axs[0].plot(position,torch.sigmoid(sigmoid_start+(sigmoid_end-sigmoid_start)*position**sigmoid_power))\n",
    "axs[0].plot(position,torch.exp(-0.5*beta_d*position**2-beta_min*position),'.')\n",
    "print(torch.exp(-0.5*beta_d*position**2-beta_min*position))\n",
    "axs[0].set_xlabel(\"time position\")\n",
    "axs[0].set_ylabel(\"alpha_t\")\n",
    "#axs[1].semilogy(position,torch.sigmoid(sigmoid_start+(sigmoid_end-sigmoid_start)*position**sigmoid_power))\n",
    "axs[1].semilogy(position,torch.exp(-0.5*beta_d*position**2-beta_min*position),'.')\n",
    "axs[1].set_xlabel(\"time position\")\n",
    "axs[1].set_ylabel(\"alpha_t\")\n",
    "plt.suptitle('Sigmoid Diffusion Scheduling')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5d6e2e-3814-4fc6-a333-99ec30cb5847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814c9c9e-e8a5-45f8-93fe-1d06f36136d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter=1\n",
    "epoch=0\n",
    "Wasserstein1= np.array([])\n",
    "Hellinger = np.array([])\n",
    "JSD = np.array([])\n",
    "Iter = np.array([])\n",
    "while (iter<=400000):\n",
    "    iter=iter+1  \n",
    "   \n",
    "    \"\"\"\n",
    "    Draw a minibatch of data from the true data distribution\n",
    "    \"\"\"  \n",
    "    x0,D = draw_minibatch(BatchSize,data_choice)\n",
    "    \n",
    "    \"\"\"\n",
    "    Define time position for each data point\n",
    "    \"\"\"  \n",
    "    flag=True\n",
    "    #while flag:\n",
    "    if 1:\n",
    "        #uniform random positions Unif(1e-5,1)\n",
    "        #current time t\n",
    "        position = 1-torch.rand(BatchSize,1)*(1-1e-5)\n",
    "        #previous time s\n",
    "        Pi = 0.95\n",
    "        position_previous = position*Pi\n",
    "        \n",
    "        #position_previous = (position-1/T).clamp(min=0)\n",
    "        #alpha_t\n",
    "        if Beta_Linear_Schedule==False:\n",
    "            logit_alpha = sigmoid_start+(sigmoid_end-sigmoid_start)*(position**sigmoid_power)\n",
    "            logit_alpha_previous = sigmoid_start+(sigmoid_end-sigmoid_start)*(position_previous**sigmoid_power)\n",
    "        else:\n",
    "            logit_alpha = (-0.5*beta_d*position**2-beta_min*position).to(torch.float64).exp().logit().to(torch.float32)\n",
    "            logit_alpha_previous = (-0.5*beta_d*position_previous**2-beta_min*position_previous).to(torch.float64).exp().logit().to(torch.float32)\n",
    "            \n",
    "        alpha = torch.sigmoid(logit_alpha)\n",
    "        alpha_previous = torch.sigmoid(logit_alpha_previous)\n",
    "        #delta = alpha_previous - alpha\n",
    "        \n",
    "        delta  = (logit_alpha_previous.to(torch.float64).sigmoid()-logit_alpha.to(torch.float64).sigmoid()).to(torch.float32)\n",
    "        \n",
    "        #eta_delta  = (eta*logit_alpha_previous.to(torch.float64).sigmoid()-eta*logit_alpha.to(torch.float64).sigmoid()).to(torch.float32)\n",
    "        \n",
    "        #embedding_pos = alpha\n",
    "        embedding_pos = position\n",
    "    \n",
    "        pe = get_positional_embedding(embed_size, embedding_pos)\n",
    "        \n",
    "        #pe = get_positional_embedding(embed_size, alpha)\n",
    "        #pe = get_positional_embedding(embed_size, -logit_alpha/8)\n",
    "              \n",
    "\n",
    "\n",
    "    if diffusion_option == \"Beta\":\n",
    "        #sample x~Beta(a,b) using x= u/(u+v), u~Gamma(a,1), v~Gamma(v,1)\n",
    "        #logit(x) = log(x/(1-x)) = log(u)-log(v), where u~Gamma(a,1), v~Gamma(v,1)\n",
    "        #log_u = log_gamma(eta*(alpha[t]*x0+epsilon).clamp(MIN,1-EPS))\n",
    "        #log_v = log_gamma(eta*(1-alpha[t]*x0).clamp(MIN,1-EPS))\n",
    "            \n",
    "        x0_Scale_Shift = x0*Scale + Shift\n",
    "        log_u = log_gamma(eta*alpha*x0_Scale_Shift)\n",
    "        log_v = log_gamma(eta-eta*alpha*x0_Scale_Shift)\n",
    "        logit_x_t = log_u - log_v\n",
    "\n",
    "        if torch.isnan(logit_x_t).any():\n",
    "            print(iter)\n",
    "            print(\"NaN in logit_x_t detected in the tensor. Stopping code.\")\n",
    "            sys.exit()\n",
    "        \n",
    "        logit_x0_hat = model(logit_x_t, pe)  \n",
    "        \n",
    "        x0_hat = torch.sigmoid(logit_x0_hat) *Scale+Shift\n",
    "\n",
    "        if torch.isnan(x0_hat).any():\n",
    "            print(iter)\n",
    "            print(\"NaN in x0_hat detected in the tensor. Stopping code.\")\n",
    "            sys.exit()\n",
    "        \n",
    "\n",
    "        alpha_p = eta*delta*x0_Scale_Shift \n",
    "        #alpha_p = eta_delta*x0_Scale_Shift \n",
    "        beta_p = eta-eta*alpha_previous*x0_Scale_Shift\n",
    "\n",
    "        alpha_q = eta*delta*x0_hat\n",
    "        #alpha_q = eta_delta*x0_hat\n",
    "        beta_q  = eta-eta*alpha_previous*x0_hat \n",
    "\n",
    "        _alpha_p = eta*alpha*x0_Scale_Shift\n",
    "        _beta_p  = eta-eta*alpha*x0_Scale_Shift\n",
    "        _alpha_q = eta*alpha*x0_hat\n",
    "        _beta_q  = eta-eta*alpha*x0_hat \n",
    "            \n",
    "            \n",
    "        #KL is nonnegative in theory, so clamp then at 0\n",
    "        #The KL between beta distributions can be written as three different KLs between gamma distributions.\n",
    "        \n",
    "        KLUB_conditional = (KL_gamma(alpha_q,alpha_p).clamp(0)\\\n",
    "                            + KL_gamma(beta_q,beta_p).clamp(0)\\\n",
    "                            - KL_gamma(alpha_q+beta_q,alpha_p+beta_p).clamp(0)).clamp(0)\n",
    "        KLUB_marginal = (KL_gamma(_alpha_q,_alpha_p).clamp(0)\\\n",
    "                            + KL_gamma(_beta_q,_beta_p).clamp(0)\\\n",
    "                            - KL_gamma(_alpha_q+_beta_q,_alpha_p+_beta_p).clamp(0)).clamp(0)\n",
    "        \n",
    "        ELBO_conditional = (KL_gamma(alpha_p,alpha_q).clamp(0)\\\n",
    "                                + KL_gamma(beta_p,beta_q).clamp(0)\\\n",
    "                                - KL_gamma(alpha_p+beta_p,alpha_q+beta_q).clamp(0)).clamp(0)\n",
    "            \n",
    "        ELBO_marginal = (KL_gamma(_alpha_p,_alpha_q).clamp(0)\\\n",
    "                                + KL_gamma(_beta_p,_beta_q).clamp(0)\\\n",
    "                                - KL_gamma(_alpha_p+_beta_p,_alpha_q+_beta_q).clamp(0)).clamp(0)\n",
    "        \n",
    "        if loss_option == \"KLUB\":\n",
    "            loss = (.5*KLUB_conditional+.5*KLUB_marginal).mean()\n",
    "        elif loss_option == \"KLUB_conditional\":\n",
    "            loss = KLUB_conditional.mean()\n",
    "        elif loss_option == \"KLUB_marginal\":            \n",
    "            loss = KLUB_marginal.mean()\n",
    "        elif loss_option == \"ELBO\": \n",
    "            loss = (0.5*ELBO_conditional+0.5*ELBO_marginal).mean()\n",
    "        elif loss_option == \"ELBO_conditional\":  \n",
    "            loss = ELBO_conditional.mean()\n",
    "        elif loss_option == \"ELBO_marginal\":  \n",
    "            loss = ELBO_marginal.mean()\n",
    "        else:\n",
    "            print(\"Invalid loss option\")\n",
    "    elif diffusion_option == \"Gauss\":\n",
    "        noise_in = torch.randn_like(x0)\n",
    "        #x_t = torch.sqrt(alpha)*x0+torch.sqrt(1.0-alpha)*noise_in\n",
    "        x_t = torch.sqrt(alpha)*x0+torch.exp(0.5*torch.log1p(-alpha))*noise_in\n",
    "        if Predict_x0:\n",
    "            x0_hat = model(x_t, pe)\n",
    "            L2 = (x0-x0_hat)**2\n",
    "        else:\n",
    "            noise_pred = model(x_t,pe) \n",
    "            #x0_hat = ((x_t - torch.sqrt(1-alpha)*noise_pred)/torch.sqrt(alpha))\n",
    "            x0_hat = ((x_t - torch.exp(0.5*torch.log1p(-alpha))*noise_pred)/torch.sqrt(alpha))\n",
    "            L2 = (noise_in-noise_pred)**2\n",
    "        if Predict_x0:\n",
    "            #KLUB_conditional = torch.exp(torch.log(delta)-torch.log(torch.tensor(2))-torch.log1p(-alpha_previous)-torch.log1p(-alpha))*L2\n",
    "            #KLUB_marginal = torch.exp(torch.log(alpha)-torch.log1p(-alpha))*L2\n",
    "            KLUB_conditional = 0.5*(logit_alpha_previous.exp()- logit_alpha.exp())*L2\n",
    "            KLUB_marginal = logit_alpha.exp()*L2\n",
    "        else:\n",
    "            #KLUB_conditional = torch.exp(torch.log(delta)-torch.log(torch.tensor(2))-torch.log1p(-alpha_previous)-torch.log(alpha))*L2\n",
    "            #KLUB_marginal = L2\n",
    "            KLUB_conditional = 0.5* (logit_alpha_previous-logit_alpha).expm1()*L2 # torch.exp(torch.log(delta)-torch.log(torch.tensor(2))-torch.log1p(-alpha_previous)-torch.log(alpha))*L2\n",
    "            KLUB_marginal = L2\n",
    "        \n",
    "        if loss_option == \"KLUB\":\n",
    "            KLUB = 0.5*KLUB_conditional+0.5*KLUB_marginal\n",
    "            loss = KLUB.mean()        \n",
    "        elif loss_option == \"KLUB_conditional\":\n",
    "            loss = KLUB_conditional.mean()\n",
    "        elif loss_option == \"KLUB_marginal\" or loss_option == \"Weighted_ELBO\":   \n",
    "            loss = KLUB_marginal.mean()\n",
    "        else:\n",
    "            print(\"Invalid loss option\")\n",
    "    else:\n",
    "        print(\"Invalid diffusion option\")\n",
    "        \n",
    "    if torch.isnan(loss).any():\n",
    "        print(\"NaN in loss detected in the tensor. Stopping code.\")\n",
    "        sys.exit()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    #gradient norm regularization\n",
    "    #nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    if (iter%1000==0 and iter<10000) or iter%10000==0:\n",
    "    #if iter>2:\n",
    "        print(iter,loss)\n",
    "        if False:\n",
    "            #Optional learning rate decay\n",
    "            scheduler.step()\n",
    "            print(\"Learning rate:\", optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "        datashape = x0.shape\n",
    "        delta = torch.cat([torch.tensor([[1.0]]),alpha[0:-1]])-alpha       \n",
    "        \n",
    "        num_steps = T\n",
    "        \n",
    "#         if diffusion_option == \"Beta\":\n",
    "#             x0_hat,z_1 = beta_reverse_sampler(model,datashape,num_steps,alpha,diffusion_option,Scale,Shift,embed_size)\n",
    "#         else:\n",
    "#             x0_hat,z_1 = gauss_reverse_sampler(model,datashape,num_steps,alpha,embed_size)\n",
    "    \n",
    "        \n",
    "        #generate 100k data points to calcuate evaluation metrics\n",
    "        Size = 100000\n",
    "        datashape = (Size,1)\n",
    "        #z_1, x0_hat = reverse_diffusion(model,datashape,TT,pe,alpha,delta,diffusion_option,alpha_min)\n",
    "        if diffusion_option == \"Beta\":\n",
    "            x0_hat,z_1 = beta_reverse_sampler(model,datashape,num_steps,alpha,diffusion_option,Scale,Shift,embed_size)\n",
    "        else:\n",
    "            x0_hat,z_1 = gauss_reverse_sampler(model,datashape,num_steps,alpha,embed_size)\n",
    "\n",
    "        \n",
    "        if data_choice==\"Type1\":\n",
    "            x0_ =D.repeat_interleave(int(datashape[0]/len(D)))\n",
    "            W1 = torch.mean(torch.abs(x0_ - torch.sort(x0_hat.squeeze()).values)).numpy()\n",
    "        else:\n",
    "            x0_,_ = draw_minibatch(Size,data_choice)\n",
    "            W1 = torch.mean(torch.abs(torch.sort(x0_.squeeze()).values - torch.sort(x0_hat.squeeze()).values)).numpy()    \n",
    "        Wasserstein1 = np.append( Wasserstein1,W1)\n",
    "        print(iter,\"Wasserstein1\",Wasserstein1)\n",
    "        Iter = np.append( Iter,iter)\n",
    "        \n",
    "        #Generated data\n",
    "        data = x0_hat.squeeze().numpy()\n",
    "        bins = np.linspace(0, 1, num=101)\n",
    "        # Create histogram\n",
    "        hist, bins = np.histogram(data, bins)\n",
    "        # Compute empirical frequency\n",
    "        q = hist / len(data)\n",
    "\n",
    "        # True data\n",
    "        data = x0_.squeeze().numpy()\n",
    "        # Create histogram\n",
    "        hist, bins = np.histogram(data, bins)\n",
    "        # Compute empirical frequency\n",
    "        p = hist / len(data)\n",
    "\n",
    "        JSD1 = np.sum(p[p>0]*np.log(p[p>0]/(p[p>0]/2.0+q[p>0]/2.0))) \n",
    "        JSD2 = np.sum(q[q>0]*np.log(q[q>0]/(p[q>0]/2.0+q[q>0]/2.0)))\n",
    "        JSDnew = JSD1/2 + JSD2/2\n",
    "\n",
    "        JSD  = np.append(JSD,JSDnew)\n",
    "        Hellinger = np.append(Hellinger, np.sqrt(np.sum((np.sqrt(p)-np.sqrt(q))**2))/np.sqrt(2))\n",
    "        \n",
    "        print(iter,\"JSD\", JSD)\n",
    "        print(iter,\"Hellinger\",Hellinger)\n",
    "        \n",
    "        epoch = epoch+1\n",
    "        if SaveCheckPoint:\n",
    "            checkpoint_path = f'results/BetaDiff_model_checkpoint_{data_choice}_{diffusion_option}_{loss_option}_{epoch}.pth'\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "        \n",
    "            results_path = f'results/BetaDiff_results_{data_choice}_{diffusion_option}_{loss_option}_{epoch}.pth'\n",
    "            torch.save((Iter,Wasserstein1, JSD,Hellinger),  results_path)\n",
    "        \n",
    "        # Set the bin edges for the histograms\n",
    "\n",
    "        bin_edges = torch.linspace(0, 1.0, 51)\n",
    "        # Create subplots\n",
    "        fig, axs = plt.subplots(3, 3, figsize=(12, 9))\n",
    "\n",
    "        # Plot the observed data\n",
    "        axs[0, 0].hist(x0.numpy(), bin_edges, density=True)\n",
    "        axs[0, 0].set_title(\"Observed Data\")\n",
    "\n",
    "        # Plot the reversed latent variable z_1 with 10 steps, random positions\n",
    "        axs[0, 1].hist(z_1.numpy(), bin_edges, density=True)\n",
    "        axs[0, 1].set_title(\"z_0\")\n",
    "\n",
    "        # Plot the reversed data estimation with 10 steps, random positions\n",
    "        axs[0, 2].hist(x0_hat.numpy(), bin_edges, density=True)\n",
    "        axs[0, 2].set_title(\"x0_hat\")\n",
    "\n",
    "        # Plot logit(x_t) in Beta and x_t in Gaussian\n",
    "        if diffusion_option == \"Beta\":\n",
    "            axs[1, 0].hist(logit_x_t.numpy())\n",
    "        else:\n",
    "            axs[1, 0].hist(x_t.numpy())\n",
    "        axs[1, 0].set_title(\"logit(x_t) in Beta, x_t in Gaussian\")\n",
    "\n",
    "\n",
    "        # Plot the Wasserstein distance\n",
    "        axs[2, 0].plot(Wasserstein1)\n",
    "        axs[2, 0].set_title(\"Wasserstein Distance\")\n",
    "\n",
    "        # Plot the JSD (Jensen-Shannon Divergence)\n",
    "        axs[2, 1].plot(JSD)\n",
    "        axs[2, 1].set_title(\"Jensen-Shannon Divergence\")\n",
    "\n",
    "        # Plot the Hellinger distance\n",
    "        axs[2, 2].plot(Hellinger)\n",
    "        axs[2, 2].set_title(\"Hellinger Distance\")\n",
    "        plt.suptitle(f'{diffusion_option}_{loss_option}_{data_choice}_Iter{iter}')\n",
    "        # Adjust spacing between subplots\n",
    "        plt.tight_layout()\n",
    "        # Show the plot\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3f605b-2784-4292-b48a-f84efed4fce7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
